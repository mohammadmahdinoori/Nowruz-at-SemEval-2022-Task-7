# -*- coding: utf-8 -*-
"""Nowruz at SemEval 2022: Tackling Cloze Tests with Transformers and Ordinal Regression

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RXkjBpzNJtc0WhhrKMjU-50rd5uSviX3
"""

import torch
import torch.nn as nn
from torch.functional import F

from datasets import Dataset

import transformers as ts
from transformers import AutoTokenizer , AutoModelForSequenceClassification
from transformers import TrainingArguments, Trainer
from transformers import DataCollatorWithPadding
from transformers import create_optimizer

from transformers.file_utils import ModelOutput
from transformers.modeling_outputs import SequenceClassifierOutput

from coral_pytorch.layers import CoralLayer
from coral_pytorch.losses import coral_loss
from coral_pytorch.dataset import levels_from_labelbatch
from coral_pytorch.dataset import proba_to_label

from dataclasses import dataclass
from typing import Optional, Tuple

import numpy as np
import pandas as pd
from scipy import stats

import sys

from data_loader import (
    retrieve_instances_from_dataset,
    retrieve_labels_from_dataset_for_classification,
    retrieve_labels_from_dataset_for_ranking,
    write_predictions_to_file,
)

"""#Preparing Data"""

def loadDataset(dataPath , labelPath=None , scoresPath=None):
  dataset = pd.read_csv(dataPath, sep="\t", quoting=3)
  
  ids , sentences , fillers = retrieve_instances_from_dataset(dataset)

  #Creating dictionaries to convert datas to Huggingface Dataset
  datasetDict = {
      "id": ids,
      "sentence": sentences,
      "filler": fillers,
  }

  labels = None
  
  if labelPath != None:
    labels = pd.read_csv(labelPath, sep="\t", header=None, names=["Id", "Label"])
    labels = retrieve_labels_from_dataset_for_classification(labels)
    datasetDict["labels"] = labels

  scores = None

  if scoresPath != None:
    scores = pd.read_csv(scoresPath, sep="\t", header=None, names=["Id", "Label"])
    scores = retrieve_labels_from_dataset_for_ranking(scores)
    datasetDict["scores"] = scores

  #Removing Periods if fillers appear at the end of the sentence (because if we don't period will be considered last word piece of the filler)
  for index , _ in enumerate(fillers):
    fillers[index].replace("." , "")

  #Creating Huggingface Datasets from Dictionaries
  dataset = Dataset.from_dict(datasetDict)

  return dataset

"""#Preprocessing"""

def preprocessDataset(dataset , tokenizer):
  def addToDict(dict_1 , dict_2 , columns_1=[] , columns_2=["input_ids" , "attention_mask"]):
    for item_1 , item_2 in zip(columns_1 , columns_2):
      dict_1[item_1] = dict_2.pop(item_2)

  def mappingFunction(dataset):
    outputDict = {}

    cleanedSentence = dataset["sentence"].replace("\n" , " ").replace("(...)" , "").strip()
    sentenceWithFiller = cleanedSentence.replace("[MASK]" , dataset["filler"].strip()).strip()

    tokenized_sentence = tokenizer(sentenceWithFiller)
    addToDict(outputDict , tokenized_sentence , ["input_ids" , "attention_mask"])

    #Getting the index of the last word piece of the filler
    if "cls_token" in tokenizer.special_tokens_map.keys():
      filler_indecies = len(tokenizer(tokenizer.special_tokens_map["cls_token"] + " " + cleanedSentence.split("[MASK]")[0].strip() + " " + dataset["filler"].strip() , add_special_tokens=False)["input_ids"]) - 1
    elif "bos_token" in tokenizer.special_tokens_map.keys():
      filler_indecies = len(tokenizer(tokenizer.special_tokens_map["bos_token"] + " " + cleanedSentence.split("[MASK]")[0].strip() + " " + dataset["filler"].strip() , add_special_tokens=False)["input_ids"]) - 1
    else:
      filler_indecies = len(tokenizer(cleanedSentence.split("[MASK]")[0].strip() + " " + dataset["filler"].strip() , add_special_tokens=False)["input_ids"]) - 1

    outputDict["filler_indecies"] = filler_indecies

    return outputDict
  
  return dataset.map(mappingFunction , batched=False)

"""#Model Definition"""

@dataclass
class CustomOutput(ModelOutput):
    loss: Optional[torch.FloatTensor] = None
    logits: torch.FloatTensor = None
    classificationOutput: torch.FloatTensor = None
    regressionOutput: torch.FloatTensor = None

class SequenceClassificationModel(nn.Module):
  def __init__(self,
               encoder,
               dim, 
               use_coral=False, 
               use_cls=True, 
               supportPooledRepresentation=False, 
               mode="both", 
               num_labels=3, 
               num_ranks=5,
               lambda_c=0.5,
               lambda_r=0.5,
               dropout_rate=0.2):
    
    super().__init__()

    #mode can be one of these: ["both" , "classification" , "regression"]

    self.encoder = encoder
    self.dim = dim
    self.use_coral = use_coral
    self.use_cls = use_cls
    self.supportPooledRepresentation = supportPooledRepresentation
    self.mode = mode
    self.num_labels = num_labels
    self.num_ranks = num_ranks
    self.lambda_c = lambda_c
    self.lambda_r = lambda_r
    self.dropout_rate = dropout_rate

    if self.use_cls:
      self.pre_classifier = nn.Linear(self.dim*2 , self.dim , bias=True)
    else:
      self.pre_classifier = nn.Linear(self.dim , self.dim , bias=True)

    self.dropout = nn.Dropout(p=self.dropout_rate , inplace=False)

    self.regressionHead = CoralLayer(self.dim , self.num_ranks)

    if use_coral:
      self.classificationHead = CoralLayer(self.dim , self.num_labels)
    else:
      self.classificationHead = nn.Linear(self.dim , self.num_labels , bias=True)
    
  def forward(
      self,
      input_ids,
      attention_mask,
      filler_indecies,
      labels=None,
      scores=None,
      **args):
    
    device = self.encoder.device

    # Getting fillers representation from pre-trained transformer (encoder)
    sentence_embedding = self.encoder(
        input_ids=input_ids,
        attention_mask=attention_mask,
    )

    #Getting Fillers Representation
    filler_tokens = sentence_embedding[0][filler_indecies[0] , filler_indecies[1]]
    fillers = filler_tokens[: , 0 , :]

    #Concatenating [CLS] output with Filler output if the model supports [CLS]
    pooled_output = None

    if self.use_cls:
      if self.supportPooledRepresentation:
        pooled_output = torch.concat((sentence_embedding[1] , fillers) , dim=-1)
      else:
        pooled_output = torch.concat((sentence_embedding[0][: , 0 , :] , fillers) , dim=-1)
    else:
      pooled_output = fillers
    
    #Passing Pooled Output to another dense layer followed by activation function and dropout
    pooled_output = self.pre_classifier(pooled_output)  
    pooled_output = nn.GELU()(pooled_output)  
    pooled_output = self.dropout(pooled_output)
    
    #Passing the final output to the classificationHead and RegressionHead
    classificationOutput = self.classificationHead(pooled_output)
    regressionOutput = self.regressionHead(pooled_output)

    totalLoss = None
    classification_loss = None
    regression_loss = None

    #Computing classification loss
    if labels != None and (self.mode.lower() == "both" or self.mode.lower() == "classification"):
      if self.use_coral:
        levels = levels_from_labelbatch(labels.view(-1) , self.num_labels).to(device)
        classification_loss = coral_loss(classificationOutput.view(-1 , self.num_labels - 1) , levels.view(-1 , self.num_labels - 1))
      else:
        loss_fct = nn.CrossEntropyLoss()
        classification_loss = loss_fct(classificationOutput.view(-1 , self.num_labels) , labels.view(-1))

    #Computing regression loss
    if scores != None and (self.mode.lower() == "both" or self.mode.lower() == "regression"):
      levels = levels_from_labelbatch(scores.view(-1) , self.num_ranks).to(device)
      regression_loss = coral_loss(regressionOutput.view(-1 , self.num_ranks - 1) , levels.view(-1 , self.num_ranks - 1))

    if self.mode.lower() == "both" and (labels != None and scores != None):
      totalLoss = (self.lambda_c * classification_loss) + (self.lambda_r * regression_loss)
    elif self.mode.lower() == "classification" and labels != None:
      totalLoss = classification_loss
    elif self.mode.lower() == "regression" and scores != None:
      totalLoss = regression_loss

    outputs = torch.concat((classificationOutput , regressionOutput) , dim=-1)

    finalClassificationOutput = torch.sigmoid(classificationOutput)
    finalRegressionOutput = torch.sigmoid(regressionOutput)

    finalClassificationOutput = proba_to_label(finalClassificationOutput.cpu().detach()).numpy()
    finalRegressionOutput = torch.sum(finalRegressionOutput.cpu().detach() , dim=-1).numpy() + 1

    return CustomOutput(
        loss=totalLoss,
        logits=outputs,
        classificationOutput=finalClassificationOutput,
        regressionOutput=finalRegressionOutput,
    )

def model_init(encoderPath=None,
               dimKey=None,
               customEncoder=None,
               customDim=None,
               mode="both",
               use_coral=True, 
               use_cls=True, 
               supportPooledRepresentation=False,
               freezeEmbedding=True, 
               num_labels=3, 
               num_ranks=5, 
               lambda_c=0.5, 
               lambda_r=0.5, 
               dropout_rate=0.2,):
  
  encoder = ts.AutoModel.from_pretrained(encoderPath) if encoderPath != None else customEncoder
  dim = encoder.config.to_dict()[dimKey] if dimKey != None else customDim

  model = SequenceClassificationModel(
                                      encoder,
                                      dim, 
                                      use_coral=use_coral, 
                                      use_cls=use_cls, 
                                      supportPooledRepresentation=supportPooledRepresentation, 
                                      mode=mode, 
                                      num_labels=num_labels, 
                                      num_ranks=num_ranks,
                                      lambda_c=lambda_c,
                                      lambda_r=lambda_r,
                                      dropout_rate=dropout_rate,
  )

  try:
    if freezeEmbedding:
      for param in model.encoder.embeddings.parameters():
        param.requires_grad = False
  except:
    print("The embedding layer name is different in this model, try to find the name of the emebdding layer and freeze it manually")
  
  return model

def makeTrainer(model, 
                trainDataset,
                data_collator,
                tokenizer, 
                outputsPath, 
                learning_rate=1.90323e-05, 
                scheduler="cosine",
                save_steps=5000,
                batch_size=8,
                num_epochs=5,
                weight_decay=0.00123974,
                roundingType="F"):

  def data_collator_fn(items , columns=[]):
    data_collator_input = {
        "input_ids": items[columns[0]],
        "attention_mask": items[columns[1]]
    }

    result = data_collator(data_collator_input)

    items[columns[0]] = result["input_ids"]
    items[columns[1]] = result["attention_mask"]

  def collate_function(items):
    outputDict = {
        key: [] for key in items[0].keys()
    }

    for item in items:
      for key in item.keys():
        outputDict[key].append(item[key])

    data_collator_fn(outputDict , ["input_ids" , "attention_mask"])

    #Removing unnecessary Items from outputDict
    columns = ["sentence" , "filler" , "id"]
    for item in columns:
      try:
        outputDict.pop(item)
      except:
        pass

    #Adding New Columns
    if "labels" in outputDict.keys():
      outputDict["labels"] = torch.tensor(outputDict.pop("labels"))
    if "scores" in outputDict.keys():
      if roundingType == "F":
        outputDict["scores"] = torch.tensor(outputDict.pop("scores") , dtype=torch.int32) - 1
      elif roundingType == "R":
        outputDict["scores"] = torch.tensor([round(score) for score in outputDict.pop("scores")] , dtype=torch.int32) - 1

    filler_indecies = torch.tensor(outputDict.pop("filler_indecies")).view(-1  , 1)
    outputDict["filler_indecies"] = (torch.arange(filler_indecies.shape[0]).view(-1 , 1) , filler_indecies)

    return outputDict 

  training_args = TrainingArguments(
      outputsPath, 
      learning_rate= learning_rate,
      lr_scheduler_type=scheduler,
      save_steps=save_steps,
      per_device_train_batch_size=batch_size,
      num_train_epochs=num_epochs,
      weight_decay=weight_decay,
      remove_unused_columns=False,
  )

  trainer = Trainer(
      model=model,
      args=training_args,
      train_dataset=trainDataset,
      tokenizer=tokenizer,
      data_collator=collate_function,
  )

  return trainer , collate_function

"""#Evaluating on Val Dataset"""

def evaluateModel(
    model, 
    dataset, 
    collate_function,
):

  model.eval()

  #Passing the inputs through model
  labels = []
  scores = []

  for item in dataset:
    sample_input = collate_function([item])
    outputs = model(input_ids=sample_input["input_ids"].to(model.encoder.device),
                    attention_mask=sample_input["attention_mask"].to(model.encoder.device),
                    filler_indecies=sample_input["filler_indecies"],
                    scores=None)

    labels.append(outputs["classificationOutput"][0])
    scores.append(outputs["regressionOutput"][0])


  #Computing Accuracy
  count = 0
  correctCount = 0

  for prediction , target in zip(labels , dataset["labels"]):
    count += 1
    correctCount += 1 if prediction == target else 0

  accuracy = (correctCount / count)

  #Computing Spearman
  scores = np.array(scores , dtype=np.float32)
  valScores = np.array(dataset["scores"] , dtype=np.float32)
  spearman = stats.spearmanr(scores.reshape(-1 , 1) , valScores.reshape(-1 , 1))

  return (labels , scores) , accuracy , spearman

"""#Making Predictions on Test Dataset"""

def predictOnTestDataset(
    model,
    dataset,
    collate_function,
    labelsPath=None,
    scoresPath=None,
):

  model.eval()

  ids = []
  classification_predictions = []
  ranking_predictions = []

  for item in dataset:
    sample_input = collate_function([item])

    outputs = model(input_ids=sample_input["input_ids"].to(model.encoder.device),
                    attention_mask=sample_input["attention_mask"].to(model.encoder.device),
                    filler_indecies=sample_input["filler_indecies"],
                    scores=None,
                    labels=None)

    ids.append(item["id"])
    classification_predictions.append(outputs["classificationOutput"][0])
    ranking_predictions.append(outputs["regressionOutput"][0])
  
  if labelsPath != None:
    open(labelsPath , mode="wb")
    write_predictions_to_file(labelsPath , ids , classification_predictions , "classification")
  
  if scoresPath != None:
    open(scoresPath , mode="wb")
    write_predictions_to_file(scoresPath , ids , ranking_predictions , "ranking")
  
  return ids , classification_predictions , ranking_predictions

"""#Inference"""

def inference(
    model,
    sentences,
    fillers,
    tokenizer,
    collate_function
):

  model.eval()

  datasetDict = {
      "sentence": sentences,
      "filler": fillers,
  }

  dataset = Dataset.from_dict(datasetDict)
  tokenizedDataset = preprocessDataset(dataset , tokenizer)
  finalInput = collate_function(tokenizedDataset)

  outputs = model(
      input_ids=finalInput["input_ids"].to(model.encoder.device),
      attention_mask=finalInput["attention_mask"].to(model.encoder.device),
      filler_indecies=finalInput["filler_indecies"],
  )

  finalLabels = []

  for item in outputs["classificationOutput"].reshape(-1):
    if item == 0:
      finalLabels.append("Implausible")
    elif item == 1:
      finalLabels.append("Neutral")
    elif item == 2:
      finalLabels.append("Plausible")

  finalLabels = np.array(finalLabels)

  return {
      "labels": finalLabels,
      "scores": outputs["regressionOutput"],
  }

